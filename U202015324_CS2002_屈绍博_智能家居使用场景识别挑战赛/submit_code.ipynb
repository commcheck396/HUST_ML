{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依赖的第三方库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "import codecs\n",
    "# 建模\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重新实现的数据读取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_readcsv(path, sep=','):\n",
    "    try:\n",
    "        lines = codecs.open(path).readlines()\n",
    "    except:\n",
    "        lines = codecs.open(path, encoding='latin-1').readlines()\n",
    "    header = lines[0].strip().split(sep)\n",
    "    content = []\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        try:\n",
    "            index = [i for i, x in enumerate(line) if x == ',']\n",
    "            if len(index) == len(header) - 1:\n",
    "                content.append(line.split(sep))\n",
    "            else:\n",
    "                line_content = []\n",
    "                index = [0] + index\n",
    "                for idx in range(len(header)-1):\n",
    "                    line_content.append(line[index[idx]:index[idx+1]].strip(sep))\n",
    "                line_content.append(line[index[len(header)-1]:].strip(sep))\n",
    "                content.append(line_content)\n",
    "        except:\n",
    "            pass\n",
    "    return pd.DataFrame(content, columns=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取了全部数据集中的数据并存入了DataFrame表格中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cus = pd.read_csv('智能家居使用场景识别挑战赛数据集/训练集/cus.csv', sep=',')\n",
    "test_cus = pd.read_csv('智能家居使用场景识别挑战赛数据集/测试集/cus.csv', sep=',')\n",
    "ans_cus = pd.read_csv('智能家居使用场景识别挑战赛数据集/测试集/cus.csv', sep=',')\n",
    "\n",
    "train_devupdate = robust_readcsv('智能家居使用场景识别挑战赛数据集/训练集/devUpdata.csv', sep=',')\n",
    "test_devupdate = robust_readcsv('智能家居使用场景识别挑战赛数据集/测试集/devUpdata.csv', sep=',')\n",
    "\n",
    "train_control = robust_readcsv('智能家居使用场景识别挑战赛数据集/训练集/control.csv', sep=',')\n",
    "test_control = robust_readcsv('智能家居使用场景识别挑战赛数据集/测试集/control.csv', sep=',')\n",
    "\n",
    "train_devlist = robust_readcsv('智能家居使用场景识别挑战赛数据集/训练集/devList.csv', sep=',')\n",
    "test_devlist = robust_readcsv('智能家居使用场景识别挑战赛数据集/测试集/devList.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分析\n",
    "将训练数据和预测数据的各个表中的数据按照 'uid' 来分组 获得特征，并重置索引 index，重新设置列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_devupdate_feat = train_devupdate.groupby('uid').agg({\n",
    "    'did': 'nunique',\n",
    "    'data': 'nunique',\n",
    "})\n",
    "train_devupdate_feat.reset_index(inplace=True)\n",
    "train_devupdate_feat.columns = ['uid', 'devupdate_did_count', 'devupdate_data_count']\n",
    "\n",
    "\n",
    "test_devupdate_feat = test_devupdate.groupby('uid').agg({\n",
    "    'did': 'nunique',\n",
    "    'data': 'nunique',\n",
    "})\n",
    "test_devupdate_feat.reset_index(inplace=True)\n",
    "test_devupdate_feat.columns = ['uid', 'devupdate_did_count', 'devupdate_data_count']\n",
    "\n",
    "\n",
    "train_control_feat = train_control.groupby('uid').agg({\n",
    "    'did': 'nunique',\n",
    "    'form': 'nunique',\n",
    "    'data': 'nunique',\n",
    "})\n",
    "train_control_feat.reset_index(inplace=True)\n",
    "train_control_feat.columns = ['uid', 'devcontrol_did_count', \n",
    "                              'devcontrol_form_count', 'devcontrol_data_count']\n",
    "\n",
    "test_control_feat = test_control.groupby('uid').agg({\n",
    "    'did': 'nunique',\n",
    "    'form': 'nunique',\n",
    "    'data': 'nunique',\n",
    "})\n",
    "test_control_feat.reset_index(inplace=True)\n",
    "test_control_feat.columns = ['uid', 'devcontrol_did_count', \n",
    "                              'devcontrol_form_count', 'devcontrol_data_count']\n",
    "\n",
    "\n",
    "\n",
    "train_devlist_feat = train_devlist.groupby('uid').agg({\n",
    "    'did': 'nunique',\n",
    "    'type': 'nunique',\n",
    "    'area': ['unique', 'nunique', 'count']\n",
    "})\n",
    "train_devlist_feat.reset_index(inplace=True)\n",
    "train_devlist_feat.columns = [x[0] + x[1] for x in train_devlist_feat.columns]\n",
    "\n",
    "test_devlist_feat = test_devlist.groupby('uid').agg({\n",
    "    'did': 'nunique',\n",
    "    'type': 'nunique',\n",
    "    'area': ['unique', 'nunique', 'count']\n",
    "})\n",
    "test_devlist_feat.reset_index(inplace=True)\n",
    "test_devlist_feat.columns = [x[0] + x[1] for x in test_devlist_feat.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拼接数据\n",
    "拼接上面得到的数据，按照 'uid' 进行整合, 即 同一个 uid 的数据才会合并到一起，放置到同一张DataFrame表格中，形成feature表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_feat = train_cus.merge(train_devlist_feat, on='uid')\n",
    "train_feat = train_feat.merge(train_control_feat, on='uid', how='left')\n",
    "train_feat = train_feat.merge(train_devupdate_feat, on='uid', how='left')\n",
    "train_feat.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "test_feat = test_cus.merge(test_devlist_feat, on='uid')\n",
    "test_feat = test_feat.merge(test_control_feat, on='uid', how='left')\n",
    "test_feat = test_feat.merge(test_devupdate_feat, on='uid', how='left')\n",
    "test_feat.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对字符串数据'areaunique'进行的特殊处理\n",
    "由于我们得到的area所有的取值：'areaunique'格式为一个string的列表，这个格式无法装载进入表格中进行处理，所以我们要对该项数据进行处理，我的处理方法是统计每个‘uid’的‘area’的每个token的字符的数量和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=400)\n",
    "train_dev_tfidf = tfidf.fit_transform(train_feat['areaunique'].apply(lambda x: ' '.join(x)))\n",
    "test_dev_tfidf = tfidf.transform(test_feat['areaunique'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述得到的 ndarray 做成 DataFrame 的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_tfidf = pd.DataFrame(train_dev_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "test_dev_tfidf = pd.DataFrame(test_dev_tfidf.toarray(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，将文本数据拼接到feature表中，形成一张包含所有数据的表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.concat([train_dev_tfidf, train_feat], axis=1)\n",
    "test_feat = pd.concat([test_dev_tfidf, test_feat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法模型\n",
    "主要训练了决策树相关模型对预测数据进行分类。\n",
    "运用了三个模型算法：随机森林，XGBoost以及LightGBM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机森林模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=2500, class_weight={0:0.89, 1:0.11}, bootstrap=False)\n",
    "clf.fit(train_feat.drop(['uid', 'label', 'areaunique'], axis=1), train_feat['label'])\n",
    "test_cus['label'] = clf.predict(test_feat.drop(['uid','areaunique'], axis=1),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(n_estimators=2000)\n",
    "clf.fit(train_feat.drop(['uid', 'label', 'areaunique'], axis=1), train_feat['label'])\n",
    "test_cus['label_xgb'] = clf.predict(test_feat.drop(['uid','areaunique'], axis=1),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LGBMClassifier(n_estimators=2500,class_weight={0:0.89, 1:0.11})\n",
    "clf.fit(train_feat.drop(['uid', 'label', 'areaunique'], axis=1), train_feat['label'])\n",
    "test_cus['label_lgbm'] = clf.predict(test_feat.drop(['uid','areaunique'], axis=1),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型融合\n",
    "利用Linear Blending融合方式对模型进行线性加权融合，取上述三个模型的分类结果并赋予权重，形成最终的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  uid  label  label_xgb  label_lgbm\n",
      "0    002026a27f38d9a3203eef2b8b7142fc      0          0           0\n",
      "1    017b9b51c033453de17d8aeab2de99c1      0          0           0\n",
      "2    030be6e9263ecfaca79598bd411eafd3      0          0           0\n",
      "3    0382f49c67a49252fd7baf7699cb2a6a      0          0           0\n",
      "4    050581a0e3a1593626c533a9e2f1577d      0          0           1\n",
      "..                                ...    ...        ...         ...\n",
      "262  f7f0d2d16e25cdfb09cac75d320c057d      0          0           0\n",
      "263  fa23bd332abee9a88e2d9a012083d50b      0          0           0\n",
      "264  fc3a5fcf2c41ac7e702a72a60c8c7deb      0          0           0\n",
      "265  fdfd6981bee55bb7ad89156e3e558a2c      0          0           0\n",
      "266  fed0db6a42c31cf9727ee372d0198921      0          0           0\n",
      "\n",
      "[267 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "ans=np.zeros(267,dtype=int)\n",
    "print(test_cus)\n",
    "for i in range(0,267):\n",
    "    row_labels = test_cus.index[i]\n",
    "    tmp=1*int(test_cus.at[row_labels,'label'])+1.4*int(test_cus.at[row_labels,'label_lgbm'])+0.6*int(test_cus.at[row_labels,'label_xgb'])\n",
    "    if(tmp>=0.6):\n",
    "        ans[i]=1\n",
    "    else:\n",
    "        ans[i]=0\n",
    "ans_cus['label']=ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将预测结果存储到csv格式文件中，形成结果文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_cus.to_csv('submit_file.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e2160534e3c01a2bf913a8f72dffe5787e98e463bb44f8b8c2a977aa23665b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
