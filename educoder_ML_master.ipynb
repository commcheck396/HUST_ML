{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_3\n",
    "import numpy as np\n",
    "class DecisionTree(object):\n",
    "    def __init__(self):\n",
    "        #决策树模型\n",
    "        self.tree = {}\n",
    "    def calcInfoGain(self, feature, label, index):\n",
    "        '''\n",
    "        计算信息增益\n",
    "        :param feature:测试用例中字典里的feature，类型为ndarray\n",
    "        :param label:测试用例中字典里的label，类型为ndarray\n",
    "        :param index:测试用例中字典里的index，即feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算信息增益。\n",
    "        :return:信息增益，类型float\n",
    "        '''\n",
    "        # 计算熵\n",
    "        def calcInfoEntropy(label):\n",
    "            '''\n",
    "            计算信息熵\n",
    "            :param label:数据集中的标签，类型为ndarray\n",
    "            :return:信息熵，类型float\n",
    "            '''\n",
    "            label_set = set(label)\n",
    "            result = 0\n",
    "            for l in label_set:\n",
    "                count = 0\n",
    "                for j in range(len(label)):\n",
    "                    if label[j] == l:\n",
    "                        count += 1\n",
    "                # 计算标签在数据集中出现的概率\n",
    "                p = count / len(label)\n",
    "                # 计算熵\n",
    "                result -= p * np.log2(p)\n",
    "            return result\n",
    "        # 计算条件熵\n",
    "        def calcHDA(feature, label, index, value):\n",
    "            '''\n",
    "            计算信息熵\n",
    "            :param feature:数据集中的特征，类型为ndarray\n",
    "            :param label:数据集中的标签，类型为ndarray\n",
    "            :param index:需要使用的特征列索引，类型为int\n",
    "            :param value:index所表示的特征列中需要考察的特征值，类型为int\n",
    "            :return:信息熵，类型float\n",
    "            '''\n",
    "            count = 0\n",
    "            # sub_feature和sub_label表示根据特征列和特征值分割出的子数据集中的特征和标签\n",
    "            sub_feature = []\n",
    "            sub_label = []\n",
    "            for i in range(len(feature)):\n",
    "                if feature[i][index] == value:\n",
    "                    count += 1\n",
    "                    sub_feature.append(feature[i])\n",
    "                    sub_label.append(label[i])\n",
    "            pHA = count / len(feature)\n",
    "            e = calcInfoEntropy(sub_label)\n",
    "            return pHA * e\n",
    "        base_e = calcInfoEntropy(label)\n",
    "        f = np.array(feature)\n",
    "        # 得到指定特征列的值的集合\n",
    "        f_set = set(f[:, index])\n",
    "        sum_HDA = 0\n",
    "        # 计算条件熵\n",
    "        for value in f_set:\n",
    "            sum_HDA += calcHDA(feature, label, index, value)\n",
    "        # 计算信息增益\n",
    "        return base_e - sum_HDA\n",
    "    # 获得信息增益最高的特征\n",
    "    def getBestFeature(self, feature, label):\n",
    "        max_infogain = 0\n",
    "        best_feature = 0\n",
    "        for i in range(len(feature[0])):\n",
    "            infogain = self.calcInfoGain(feature, label, i)\n",
    "            if infogain > max_infogain:\n",
    "                max_infogain = infogain\n",
    "                best_feature = i\n",
    "        return best_feature\n",
    "    def createTree(self, feature, label):\n",
    "        # 样本里都是同一个label没必要继续分叉了\n",
    "        if len(set(label)) == 1:\n",
    "            return label[0]\n",
    "        # 样本中只有一个特征或者所有样本的特征都一样的话就看哪个label的票数高\n",
    "        if len(feature[0]) == 1 or len(np.unique(feature, axis=0)) == 1:\n",
    "            vote = {}\n",
    "            for l in label:\n",
    "                if l in vote.keys():\n",
    "                    vote[l] += 1\n",
    "                else:\n",
    "                    vote[l] = 1\n",
    "            max_count = 0\n",
    "            vote_label = None\n",
    "            for k, v in vote.items():\n",
    "                if v > max_count:\n",
    "                    max_count = v\n",
    "                    vote_label = k\n",
    "            return vote_label\n",
    "        # 根据信息增益拿到特征的索引\n",
    "        best_feature = self.getBestFeature(feature, label)\n",
    "        tree = {best_feature: {}}\n",
    "        f = np.array(feature)\n",
    "        # 拿到bestfeature的所有特征值\n",
    "        f_set = set(f[:, best_feature])\n",
    "        # 构建对应特征值的子样本集sub_feature, sub_label\n",
    "        for v in f_set:\n",
    "            sub_feature = []\n",
    "            sub_label = []\n",
    "            for i in range(len(feature)):\n",
    "                if feature[i][best_feature] == v:\n",
    "                    sub_feature.append(feature[i])\n",
    "                    sub_label.append(label[i])\n",
    "            # 递归构建决策树\n",
    "            tree[best_feature][v] = self.createTree(sub_feature, sub_label)\n",
    "        return tree\n",
    "    def fit(self, feature, label):\n",
    "        '''\n",
    "        :param feature: 训练集数据，类型为ndarray\n",
    "        :param label:训练集标签，类型为ndarray\n",
    "        :return: None\n",
    "        '''\n",
    "        #************* Begin ************#\n",
    "        self.tree = self.createTree(feature, label)\n",
    "        #************* End **************#\n",
    "    _ii = 0\n",
    "    _key= 0\n",
    "    _result = [0]*45\n",
    "    def predict(self, feature):\n",
    "        '''\n",
    "        :param feature:测试集数据，类型为ndarray\n",
    "        :return:预测结果，如np.array([0, 1, 2, 2, 1, 0])\n",
    "        '''\n",
    "        #************* Begin ************#\n",
    "        result = []\n",
    "        print(self.tree)\n",
    "        print(feature)\n",
    "        def classify(tree, feature):\n",
    "            if not isinstance(tree, dict):\n",
    "                return tree\n",
    "            t_index, t_value = list(tree.items())[0]\n",
    "            f_value = feature[t_index]\n",
    "            if isinstance(t_value, dict):\n",
    "                classLabel = classify(tree[t_index][f_value], feature)\n",
    "                return classLabel\n",
    "            else:\n",
    "                return t_value\n",
    "\n",
    "        for f in feature:\n",
    "            result.append(classify(self.tree, f))\n",
    "\n",
    "        return np.array(result)\n",
    "        # print(self.tree)\n",
    "        # # print(type(self.tree[2])) DecisionTree._ii DecisionTree._key DecisionTree._result\n",
    "        # # print(type(self.tree))\n",
    "        # # print(feature)\n",
    "        # # print(np.shape(feature)[0])\n",
    "        \n",
    "        # if(isinstance(self.tree,dict)==0):\n",
    "        #     return self.tree\n",
    "        # #根节点\n",
    "        # if(len(self.tree.keys())==1):\n",
    "        #     DecisionTree._key = tuple(self.tree.keys())[0]\n",
    "        #     # print(type(key))\n",
    "        # else:\n",
    "        #     DecisionTree._key = feature[DecisionTree._ii][DecisionTree._key]\n",
    "        # print(DecisionTree._key)\n",
    "        # self.tree = self.tree[DecisionTree._key]\n",
    "        # if(isinstance(self.tree,dict)):\n",
    "        #     DecisionTree._result[DecisionTree._ii] = self.predict(feature)\n",
    "        # DecisionTree._ii+=1\n",
    "        # return DecisionTree._result\n",
    "\n",
    "\n",
    "        #************* End **************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_4\n",
    "import numpy as np\n",
    "\n",
    "def calcInfoGain(feature, label, index):\n",
    "    '''\n",
    "    计算信息增益\n",
    "    :param feature:测试用例中字典里的feature，类型为ndarray\n",
    "    :param label:测试用例中字典里的label，类型为ndarray\n",
    "    :param index:测试用例中字典里的index，即feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算信息增益。\n",
    "    :return:信息增益，类型float\n",
    "    '''\n",
    "    # 计算熵\n",
    "    def calcInfoEntropy(label):\n",
    "        '''\n",
    "        计算信息熵\n",
    "        :param label:数据集中的标签，类型为ndarray\n",
    "        :return:信息熵，类型float\n",
    "        '''\n",
    "\n",
    "        label_set = set(label)\n",
    "        result = 0\n",
    "        for l in label_set:\n",
    "            count = 0\n",
    "            for j in range(len(label)):\n",
    "                if label[j] == l:\n",
    "                    count += 1\n",
    "            # 计算标签在数据集中出现的概率\n",
    "            p = count / len(label)\n",
    "            # 计算熵\n",
    "            result -= p * np.log2(p)\n",
    "        return result\n",
    "\n",
    "    # 计算条件熵\n",
    "    def calcHDA(feature, label, index, value):\n",
    "        '''\n",
    "        计算信息熵\n",
    "        :param feature:数据集中的特征，类型为ndarray\n",
    "        :param label:数据集中的标签，类型为ndarray\n",
    "        :param index:需要使用的特征列索引，类型为int\n",
    "        :param value:index所表示的特征列中需要考察的特征值，类型为int\n",
    "        :return:信息熵，类型float\n",
    "        '''\n",
    "        count = 0\n",
    "        # sub_label表示根据特征列和特征值分割出的子数据集中的标签\n",
    "        sub_label = []\n",
    "        for i in range(len(feature)):\n",
    "            if feature[i][index] == value:\n",
    "                count += 1\n",
    "                sub_label.append(label[i])\n",
    "        pHA = count / len(feature)\n",
    "        e = calcInfoEntropy(sub_label)\n",
    "        return pHA * e\n",
    "\n",
    "    base_e = calcInfoEntropy(label)\n",
    "    f = np.array(feature)\n",
    "    # 得到指定特征列的值的集合\n",
    "    f_set = set(f[:, index])\n",
    "    sum_HDA = 0\n",
    "    # 计算条件熵\n",
    "    for value in f_set:\n",
    "        sum_HDA += calcHDA(feature, label, index, value)\n",
    "    # 计算信息增益\n",
    "    return base_e - sum_HDA\n",
    "\n",
    "\n",
    "def calcInfoGainRatio(feature, label, index):\n",
    "    '''\n",
    "    计算信息增益率\n",
    "    :param feature:测试用例中字典里的feature，类型为ndarray\n",
    "    :param label:测试用例中字典里的label，类型为ndarray\n",
    "    :param index:测试用例中字典里的index，即feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算信息增益。\n",
    "    :return:信息增益率，类型float\n",
    "    '''\n",
    "\n",
    "    #********* Begin *********#\n",
    "    infoGain = calcInfoGain(feature, label, index)\n",
    "    unique_value = list(set(feature[:, index]))\n",
    "    IV_ = 0\n",
    "    for value in unique_value:\n",
    "        len_v = np.sum(feature[:, index] == value)\n",
    "        IV_ -= (len_v/len(feature))*np.log2((len_v/len(feature)))\n",
    "    return infoGain/IV_\n",
    "    #********* End *********#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_5\n",
    "import numpy as np\n",
    "\n",
    "def calcGini(feature, label, index):\n",
    "    '''\n",
    "    计算基尼系数\n",
    "    :param feature:测试用例中字典里的feature，类型为ndarray\n",
    "    :param label:测试用例中字典里的label，类型为ndarray\n",
    "    :param index:测试用例中字典里的index，即feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算信息增益。\n",
    "    :return:基尼系数，类型float\n",
    "    '''\n",
    "\n",
    "    #********* Begin *********#\n",
    "    def _gini(label):\n",
    "        uLabel = list(set(label))\n",
    "        gini = 1\n",
    "        for l in uLabel:\n",
    "            p0 = np.sum(label == l)/len(label)\n",
    "            gini -= p0**2\n",
    "        return gini\n",
    "    uValue = list(set(feature[:, index]))\n",
    "    gini = 0\n",
    "    for value in uValue:\n",
    "        len_v = np.sum(feature[:, index] == value)\n",
    "        gini += (len_v/len(feature))*_gini(label[feature[:, index] == value])\n",
    "    return gini\n",
    "    #********* End *********#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_6\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "class DecisionTree(object):\n",
    "    def __init__(self):\n",
    "        #决策树模型\n",
    "        self.tree = {}\n",
    "    def calcInfoGain(self, feature, label, index):\n",
    "        '''\n",
    "        计算信息增益\n",
    "        :param feature:测试用例中字典里的feature，类型为ndarray\n",
    "        :param label:测试用例中字典里的label，类型为ndarray\n",
    "        :param index:测试用例中字典里的index，即feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算信息增益。\n",
    "        :return:信息增益，类型float\n",
    "        '''\n",
    "        # 计算熵\n",
    "        def calcInfoEntropy(feature, label):\n",
    "            '''\n",
    "            计算信息熵\n",
    "            :param feature:数据集中的特征，类型为ndarray\n",
    "            :param label:数据集中的标签，类型为ndarray\n",
    "            :return:信息熵，类型float\n",
    "            '''\n",
    "            label_set = set(label)\n",
    "            result = 0\n",
    "            for l in label_set:\n",
    "                count = 0\n",
    "                for j in range(len(label)):\n",
    "                    if label[j] == l:\n",
    "                        count += 1\n",
    "                # 计算标签在数据集中出现的概率\n",
    "                p = count / len(label)\n",
    "                # 计算熵\n",
    "                result -= p * np.log2(p)\n",
    "            return result\n",
    "        # 计算条件熵\n",
    "        def calcHDA(feature, label, index, value):\n",
    "            '''\n",
    "            计算信息熵\n",
    "            :param feature:数据集中的特征，类型为ndarray\n",
    "            :param label:数据集中的标签，类型为ndarray\n",
    "            :param index:需要使用的特征列索引，类型为int\n",
    "            :param value:index所表示的特征列中需要考察的特征值，类型为int\n",
    "            :return:信息熵，类型float\n",
    "            '''\n",
    "            count = 0\n",
    "            # sub_feature和sub_label表示根据特征列和特征值分割出的子数据集中的特征和标签\n",
    "            sub_feature = []\n",
    "            sub_label = []\n",
    "            for i in range(len(feature)):\n",
    "                if feature[i][index] == value:\n",
    "                    count += 1\n",
    "                    sub_feature.append(feature[i])\n",
    "                    sub_label.append(label[i])\n",
    "            pHA = count / len(feature)\n",
    "            e = calcInfoEntropy(sub_feature, sub_label)\n",
    "            return pHA * e\n",
    "        base_e = calcInfoEntropy(feature, label)\n",
    "        f = np.array(feature)\n",
    "        # 得到指定特征列的值的集合\n",
    "        f_set = set(f[:, index])\n",
    "        sum_HDA = 0\n",
    "        # 计算条件熵\n",
    "        for value in f_set:\n",
    "            sum_HDA += calcHDA(feature, label, index, value)\n",
    "        # 计算信息增益\n",
    "        return base_e - sum_HDA\n",
    "    # 获得信息增益最高的特征\n",
    "    def getBestFeature(self, feature, label):\n",
    "        max_infogain = 0\n",
    "        best_feature = 0\n",
    "        for i in range(len(feature[0])):\n",
    "            infogain = self.calcInfoGain(feature, label, i)\n",
    "            if infogain > max_infogain:\n",
    "                max_infogain = infogain\n",
    "                best_feature = i\n",
    "        return best_feature\n",
    "    # 计算验证集准确率\n",
    "    def calc_acc_val(self, the_tree, val_feature, val_label):\n",
    "        result = []\n",
    "        def classify(tree, feature):\n",
    "            if not isinstance(tree, dict):\n",
    "                return tree\n",
    "            t_index, t_value = list(tree.items())[0]\n",
    "            f_value = feature[t_index]\n",
    "            if isinstance(t_value, dict):\n",
    "                classLabel = classify(tree[t_index][f_value], feature)\n",
    "                return classLabel\n",
    "            else:\n",
    "                return t_value\n",
    "        for f in val_feature:\n",
    "            result.append(classify(the_tree, f))\n",
    "        result = np.array(result)\n",
    "        return np.mean(result == val_label)\n",
    "    def createTree(self, train_feature, train_label):\n",
    "        # 样本里都是同一个label没必要继续分叉了\n",
    "        if len(set(train_label)) == 1:\n",
    "            return train_label[0]\n",
    "        # 样本中只有一个特征或者所有样本的特征都一样的话就看哪个label的票数高\n",
    "        if len(train_feature[0]) == 1 or len(np.unique(train_feature, axis=0)) == 1:\n",
    "            vote = {}\n",
    "            for l in train_label:\n",
    "                if l in vote.keys():\n",
    "                    vote[l] += 1\n",
    "                else:\n",
    "                    vote[l] = 1\n",
    "            max_count = 0\n",
    "            vote_label = None\n",
    "            for k, v in vote.items():\n",
    "                if v > max_count:\n",
    "                    max_count = v\n",
    "                    vote_label = k\n",
    "            return vote_label\n",
    "        # 根据信息增益拿到特征的索引\n",
    "        best_feature = self.getBestFeature(train_feature, train_label)\n",
    "        tree = {best_feature: {}}\n",
    "        f = np.array(train_feature)\n",
    "        # 拿到bestfeature的所有特征值\n",
    "        f_set = set(f[:, best_feature])\n",
    "        # 构建对应特征值的子样本集sub_feature, sub_label\n",
    "        for v in f_set:\n",
    "            sub_feature = []\n",
    "            sub_label = []\n",
    "            for i in range(len(train_feature)):\n",
    "                if train_feature[i][best_feature] == v:\n",
    "                    sub_feature.append(train_feature[i])\n",
    "                    sub_label.append(train_label[i])\n",
    "            # 递归构建决策树\n",
    "            tree[best_feature][v] = self.createTree(sub_feature, sub_label)\n",
    "        return tree\n",
    "    # 后剪枝\n",
    "    def post_cut(self, val_feature, val_label):\n",
    "        # 拿到非叶子节点的数量\n",
    "        def get_non_leaf_node_count(tree):\n",
    "            non_leaf_node_path = []\n",
    "            def dfs(tree, path, all_path):\n",
    "                for k in tree.keys():\n",
    "                    if isinstance(tree[k], dict):\n",
    "                        path.append(k)\n",
    "                        dfs(tree[k], path, all_path)\n",
    "                        if len(path) > 0:\n",
    "                            path.pop()\n",
    "                    else:\n",
    "                        all_path.append(path[:])\n",
    "            dfs(tree, [], non_leaf_node_path)\n",
    "            unique_non_leaf_node = []\n",
    "            for path in non_leaf_node_path:\n",
    "                isFind = False\n",
    "                for p in unique_non_leaf_node:\n",
    "                    if path == p:\n",
    "                        isFind = True\n",
    "                        break\n",
    "                if not isFind:\n",
    "                    unique_non_leaf_node.append(path)\n",
    "            return len(unique_non_leaf_node)\n",
    "        # 拿到树中深度最深的从根节点到非叶子节点的路径\n",
    "        def get_the_most_deep_path(tree):\n",
    "            non_leaf_node_path = []\n",
    "            def dfs(tree, path, all_path):\n",
    "                for k in tree.keys():\n",
    "                    if isinstance(tree[k], dict):\n",
    "                        path.append(k)\n",
    "                        dfs(tree[k], path, all_path)\n",
    "                        if len(path) > 0:\n",
    "                            path.pop()\n",
    "                    else:\n",
    "                        all_path.append(path[:])\n",
    "            dfs(tree, [], non_leaf_node_path)\n",
    "            max_depth = 0\n",
    "            result = None\n",
    "            for path in non_leaf_node_path:\n",
    "                if len(path) > max_depth:\n",
    "                    max_depth = len(path)\n",
    "                    result = path\n",
    "            return result\n",
    "        # 剪枝\n",
    "        def set_vote_label(tree, path, label):\n",
    "            for i in range(len(path)-1):\n",
    "                tree = tree[path[i]]\n",
    "            tree[path[len(path)-1]] = vote_label\n",
    "        acc_before_cut = self.calc_acc_val(self.tree, val_feature, val_label)\n",
    "        # 遍历所有非叶子节点\n",
    "        for _ in range(get_non_leaf_node_count(self.tree)):\n",
    "            path = get_the_most_deep_path(self.tree)\n",
    "            # 备份树\n",
    "            tree = deepcopy(self.tree)\n",
    "            step = deepcopy(tree)\n",
    "            # 跟着路径走\n",
    "            for k in path:\n",
    "                step = step[k]\n",
    "            # 叶子节点中票数最多的标签\n",
    "            vote_label = sorted(step.items(), key=lambda item: item[1], reverse=True)[0][0]\n",
    "            # 在备份的树上剪枝\n",
    "            set_vote_label(tree, path, vote_label)\n",
    "            acc_after_cut = self.calc_acc_val(tree, val_feature, val_label)\n",
    "            # 验证集准确率高于0.9才剪枝\n",
    "            if acc_after_cut > acc_before_cut:\n",
    "                set_vote_label(self.tree, path, vote_label)\n",
    "                acc_before_cut = acc_after_cut\n",
    "    def fit(self, train_feature, train_label, val_feature, val_label):\n",
    "        '''\n",
    "        :param train_feature:训练集数据，类型为ndarray\n",
    "        :param train_label:训练集标签，类型为ndarray\n",
    "        :param val_feature:验证集数据，类型为ndarray\n",
    "        :param val_label:验证集标签，类型为ndarray\n",
    "        :return: None\n",
    "        '''\n",
    "        #************* Begin ************#\n",
    "        self.tree = self.createTree(train_feature, train_label)\n",
    "        # 后剪枝\n",
    "        self.post_cut(val_feature, val_label)\n",
    "        \n",
    "\n",
    "        #************* End **************#\n",
    "    def predict(self, feature):\n",
    "        '''\n",
    "        :param feature:测试集数据，类型为ndarray\n",
    "        :return:预测结果，如np.array([0, 1, 2, 2, 1, 0])\n",
    "        '''\n",
    "        #************* Begin ************#\n",
    "        result = []\n",
    "\n",
    "        # 单个样本分类\n",
    "        def classify(tree, feature):\n",
    "            if not isinstance(tree, dict):\n",
    "                return tree\n",
    "            t_index, t_value = list(tree.items())[0]\n",
    "            f_value = feature[t_index]\n",
    "            if isinstance(t_value, dict):\n",
    "                classLabel = classify(tree[t_index][f_value], feature)\n",
    "                return classLabel\n",
    "            else:\n",
    "                return t_value\n",
    "\n",
    "        for f in feature:\n",
    "            result.append(classify(self.tree, f))\n",
    "\n",
    "        return np.array(result)\n",
    "        # 单个样本分类\n",
    "\n",
    "\n",
    "\n",
    "        #************* End **************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './step7/train_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6236/1144502298.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./step7/train_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./step7/train_label.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./step7/test_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\13226\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\13226\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\13226\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\13226\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\13226\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\13226\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\13226\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\13226\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './step7/train_data.csv'"
     ]
    }
   ],
   "source": [
    "# task_7\n",
    "#********* Begin *********#\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "train_df = pd.read_csv('./step7/train_data.csv').as_matrix()\n",
    "train_label = pd.read_csv('./step7/train_label.csv').as_matrix()\n",
    "test_df = pd.read_csv('./step7/test_data.csv').as_matrix()\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(train_df, train_label)\n",
    "result = dt.predict(test_df)\n",
    "\n",
    "result = pd.DataFrame({'target':result})\n",
    "result.to_csv('./step7/predict.csv', index=False)\n",
    "\n",
    "#********* End *********#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_4\n",
    "#encoding=utf8\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#实现核函数\n",
    "def kernel(x,sigma=1.0):\n",
    "    '''\n",
    "    input:x(ndarray):样本\n",
    "    output:x(ndarray):转化后的值\n",
    "    '''    \n",
    "    #********* Begin *********#\n",
    "    z=[[0,0],[0,1],[1,0],[1,1]]\n",
    "    # print(x*z)\n",
    "    # print(x)\n",
    "    return np.exp(-sigma * np.sum((x[...,None,:] - z) ** 2, axis=2))\n",
    "    #********* End *********#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_5\n",
    "#encoding=utf8\n",
    "import numpy as np\n",
    "class SVM:\n",
    "    def __init__(self, max_iter=100, kernel='linear'):\n",
    "        '''\n",
    "        input:max_iter(int):最大训练轮数\n",
    "              kernel(str):核函数，等于'linear'表示线性，等于'poly'表示多项式\n",
    "        '''\n",
    "        self.max_iter = max_iter\n",
    "        self._kernel = kernel\n",
    "    #初始化模型\n",
    "    def init_args(self, features, labels):\n",
    "        self.m, self.n = features.shape\n",
    "        self.X = features\n",
    "        self.Y = labels\n",
    "        self.b = 0.0\n",
    "\n",
    "        # print(features)\n",
    "        # print(labels)\n",
    "        # print(features.shape)\n",
    "        # print(labels.shape)\n",
    "        # 将Ei保存在一个列表里\n",
    "        self.alpha = np.ones(self.m)\n",
    "        self.E = [self._E(i) for i in range(self.m)]\n",
    "        # 松弛变量\n",
    "        self.C = 1.0\n",
    "    #********* Begin *********#  \n",
    "    # def fit(self,features,labels):\n",
    "    #     self.init_args(features,labels)\n",
    "    #     self.alpha = \n",
    "    #kkt条件    \n",
    "    def _KKT(self, i):\n",
    "        gy = self._g(i)*self.Y[i]\n",
    "        if self.alpha[i] == 0:\n",
    "            return gy >= 1\n",
    "        elif 0 < self.alpha[i] < self.C:\n",
    "            return gy == 1\n",
    "        else:\n",
    "            return gy <= 1   \n",
    "    # g(x)预测值，输入xi（X[i]）\n",
    "    def _g(self, i):\n",
    "        r = self.b\n",
    "        for j in range(self.m):\n",
    "            r += self.alpha[j]*self.Y[j]*self.kernel(self.X[i], self.X[j])\n",
    "        return r\n",
    "    # 核函数\n",
    "    def kernel(self, x1, x2):\n",
    "        if self._kernel == 'linear':\n",
    "            return sum([x1[k]*x2[k] for k in range(self.n)])\n",
    "        elif self._kernel == 'poly':\n",
    "            return (sum([x1[k]*x2[k] for k in range(self.n)]) + 1)**2    \n",
    "        return 0\n",
    "    # E（x）为g(x)对输入x的预测值和y的差\n",
    "    def _E(self, i):\n",
    "        return self._g(i) - self.Y[i]\n",
    "    #初始alpha\n",
    "    def _init_alpha(self):\n",
    "         # 外层循环首先遍历所有满足0<a<C的样本点，检验是否满足KKT\n",
    "        index_list = [i for i in range(self.m) if 0 < self.alpha[i] < self.C]\n",
    "        # 否则遍历整个训练集\n",
    "        non_list = [i for i in range(self.m) if i not in index_list]\n",
    "        index_list.extend(non_list)\n",
    "        for i in index_list:\n",
    "            if self._KKT(i):\n",
    "                continue\n",
    "            E1 = self.E[i]\n",
    "            # 如果E2是+，选择最小的；如果E2是负的，选择最大的\n",
    "            if E1 >= 0:\n",
    "                j = min(range(self.m), key=lambda x: self.E[x])\n",
    "            else:\n",
    "                j = max(range(self.m), key=lambda x: self.E[x])\n",
    "            return i, j\n",
    "    #选择参数   \n",
    "    def _compare(self, _alpha, Low, High):\n",
    "        if _alpha > High:\n",
    "            return High\n",
    "        elif _alpha < Low:\n",
    "            return Low\n",
    "        else:\n",
    "            return _alpha\n",
    "    #训练\n",
    "    def fit(self, features, labels):\n",
    "        '''\n",
    "        input:features(ndarray):特征\n",
    "              label(ndarray):标签\n",
    "        '''\n",
    "        self.init_args(features, labels)\n",
    "        for t in range(self.max_iter):\n",
    "            i1, i2 = self._init_alpha()\n",
    "            # 边界\n",
    "            if self.Y[i1] == self.Y[i2]:\n",
    "                Low = max(0, self.alpha[i1]+self.alpha[i2]-self.C)\n",
    "                High = min(self.C, self.alpha[i1]+self.alpha[i2])\n",
    "            else:\n",
    "                Low = max(0, self.alpha[i2]-self.alpha[i1])\n",
    "                High = min(self.C, self.C+self.alpha[i2]-self.alpha[i1])\n",
    "            E1 = self.E[i1]\n",
    "            E2 = self.E[i2]\n",
    "            # eta=K11+K22-2K12\n",
    "            eta = self.kernel(self.X[i1], self.X[i1]) + self.kernel(self.X[i2], self.X[i2]) - 2*self.kernel(self.X[i1], self.X[i2])\n",
    "            if eta <= 0:\n",
    "                continue\n",
    "            alpha2_new_unc = self.alpha[i2] + self.Y[i2] * (E2 - E1) / eta\n",
    "            alpha2_new = self._compare(alpha2_new_unc, Low, High)\n",
    "            alpha1_new = self.alpha[i1] + self.Y[i1] * self.Y[i2] * (self.alpha[i2] - alpha2_new)\n",
    "            b1_new = -E1 - self.Y[i1] * self.kernel(self.X[i1], self.X[i1]) * (alpha1_new-self.alpha[i1]) - self.Y[i2] * self.kernel(self.X[i2], self.X[i1]) * (alpha2_new-self.alpha[i2])+ self.b \n",
    "            b2_new = -E2 - self.Y[i1] * self.kernel(self.X[i1], self.X[i2]) * (alpha1_new-self.alpha[i1]) - self.Y[i2] * self.kernel(self.X[i2], self.X[i2]) * (alpha2_new-self.alpha[i2])+ self.b \n",
    "            if 0 < alpha1_new < self.C:\n",
    "                b_new = b1_new\n",
    "            elif 0 < alpha2_new < self.C:\n",
    "                b_new = b2_new\n",
    "            else:\n",
    "                # 选择中点\n",
    "                b_new = (b1_new + b2_new) / 2\n",
    "            # 更新参数\n",
    "            self.alpha[i1] = alpha1_new\n",
    "            self.alpha[i2] = alpha2_new\n",
    "            self.b = b_new\n",
    "            self.E[i1] = self._E(i1)\n",
    "            self.E[i2] = self._E(i2)\n",
    "    #********* End *********#        \n",
    "    def predict(self, data):\n",
    "        r = self.b\n",
    "        for i in range(self.m):\n",
    "            r += self.alpha[i] * self.Y[i] * self.kernel(data, self.X[i])\n",
    "        return 1 if r > 0 else -1\n",
    "    def score(self, X_test, y_test):\n",
    "        right_count = 0\n",
    "        for i in range(len(X_test)):\n",
    "            result = self.predict(X_test[i])\n",
    "            if result == y_test[i]:\n",
    "                right_count += 1\n",
    "        return right_count / len(X_test)\n",
    "    def _weight(self):\n",
    "        yx = self.Y.reshape(-1, 1)*self.X\n",
    "        self.w = np.dot(yx.T, self.alpha)\n",
    "        return self.w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_6\n",
    "#encoding=utf8\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm_classifier(train_data,train_label,test_data):\n",
    "    '''\n",
    "    input:train_data(ndarray):训练样本\n",
    "          train_label(ndarray):训练标签\n",
    "          test_data(ndarray):测试样本\n",
    "    output:predict(ndarray):预测结果      \n",
    "    '''\n",
    "    #********* Begin *********#\n",
    "    svc = SVC()  \n",
    "    svc.fit(train_data,train_label)  \n",
    "    predict = svc.predict(test_data) \n",
    "    #********* End *********#\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_1\n",
    "#encoding=utf8\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(t):\n",
    "    '''\n",
    "    完成sigmoid函数计算\n",
    "    :param t: 负无穷到正无穷的实数\n",
    "    :return: 转换后的概率值\n",
    "    :可以考虑使用np.exp()函数\n",
    "    '''\n",
    "    #********** Begin **********#\n",
    "    return 1/(1+np.exp(-t))\n",
    "    #********** End **********#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    def gradient_descent(initial_theta,eta=0.05,n_iters=1000,epslion=1e-8):\n",
    "        '''\n",
    "        梯度下降\n",
    "        :param initial_theta: 参数初始值，类型为float\n",
    "        :param eta: 学习率，类型为float\n",
    "        :param n_iters: 训练轮数，类型为int\n",
    "        :param epslion: 容忍误差范围，类型为float\n",
    "        :return: 训练后得到的参数\n",
    "        '''\n",
    "        #   请在此添加实现代码   #\n",
    "        #********** Begin *********#\n",
    "        # print(initial_theta)\n",
    "        for i in range(n_iters):\n",
    "            initial_theta = initial_theta + eta*2\n",
    "            if(abs(2*(initial_theta-3))<=epslion):\n",
    "                return initial_theta\n",
    "        return initial_theta\n",
    "    #********** End **********#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_4\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "    sigmoid函数\n",
    "    :param x: 转换前的输入\n",
    "    :return: 转换后的概率\n",
    "    '''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "def fit(x,y,eta=1e-3,n_iters=10000):\n",
    "    '''\n",
    "    训练逻辑回归模型\n",
    "    :param x: 训练集特征数据，类型为ndarray\n",
    "    :param y: 训练集标签，类型为ndarray\n",
    "    :param eta: 学习率，类型为float\n",
    "    :param n_iters: 训练轮数，类型为int\n",
    "    :return: 模型参数，类型为ndarray\n",
    "    '''\n",
    "    #   请在此添加实现代码   #\n",
    "    #********** Begin *********#\n",
    "    # print(x)\n",
    "    # print(x.shape(1))\n",
    "    # print(np.size(y))\n",
    "    w = [1]*31\n",
    "    # print(w)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        for j in range(np.size(y)):\n",
    "            z = sum(w*x[j])\n",
    "            y0 = (1 if sigmoid(z)>=0.5 else 0)\n",
    "            if(y0!=y[j]):\n",
    "                w = w - eta*(y0-y[j])*x[j]\n",
    "    return w\n",
    "    #********** End **********#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_5\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def digit_predict(train_image, train_label, test_image):\n",
    "    '''\n",
    "    实现功能：训练模型并输出预测结果\n",
    "    :param train_sample: 包含多条训练样本的样本集，类型为ndarray,shape为[-1, 8, 8]\n",
    "    :param train_label: 包含多条训练样本标签的标签集，类型为ndarray\n",
    "    :param test_sample: 包含多条测试样本的测试集，类型为ndarry\n",
    "    :return: test_sample对应的预测标签\n",
    "    '''\n",
    "\n",
    "    #************* Begin ************#\n",
    "    logreg = LogisticRegression(solver='sag',max_iter =100,C=10) \n",
    "    # print(np.shape(train_image)) \n",
    "    # print(np.shape(test_image)) \n",
    "    # print(np.size(train_label)) \n",
    "    # print(train_image[1])\n",
    "    train_sample = [[0]*64]*np.size(train_label)\n",
    "    # print(np.shape(train_sample)) \n",
    "    # print(np.shape(train_image[1].flatten())) \n",
    "    test_sample = [[0]*64]*360\n",
    "    for i in range(np.size(train_label)):\n",
    "        train_sample[i] = train_image[i].flatten()\n",
    "    for j in range(360):\n",
    "        test_sample[j] = test_image[j].flatten()\n",
    "    logreg.fit(train_sample,train_label)  \n",
    "    result = logreg.predict(test_sample)\n",
    "    return result\n",
    "    #************* End **************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_6236/1439450578.py, line 91)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\13226\\AppData\\Local\\Temp/ipykernel_6236/1439450578.py\"\u001b[1;36m, line \u001b[1;32m91\u001b[0m\n\u001b[1;33m    a2 = self.label_prob[1]*(self.condition_prob[1][0][feature[i][0]])*(self.confrom sklearn.linear_model import LogisticRegression\u001b[0m\n\u001b[1;37m                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# task_3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier(object):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        self.label_prob表示每种类别在数据中出现的概率\n",
    "        例如，{0:0.333, 1:0.667}表示数据中类别0出现的概率为0.333，类别1的概率为0.667\n",
    "        '''\n",
    "        self.label_prob = {}\n",
    "        '''\n",
    "        self.condition_prob表示每种类别确定的条件下各个特征出现的概率\n",
    "        例如训练数据集中的特征为 [[2, 1, 1],\n",
    "                              [1, 2, 2],\n",
    "                              [2, 2, 2],\n",
    "                              [2, 1, 2],\n",
    "                              [1, 2, 3]]\n",
    "        标签为[1, 0, 1, 0, 1]\n",
    "        那么当标签为0时第0列的值为1的概率为0.5，值为2的概率为0.5;\n",
    "        当标签为0时第1列的值为1的概率为0.5，值为2的概率为0.5;\n",
    "        当标签为0时第2列的值为1的概率为0，值为2的概率为1，值为3的概率为0;\n",
    "        当标签为1时第0列的值为1的概率为0.333，值为2的概率为0.666;\n",
    "        当标签为1时第1列的值为1的概率为0.333，值为2的概率为0.666;\n",
    "        当标签为1时第2列的值为1的概率为0.333，值为2的概率为0.333,值为3的概率为0.333;\n",
    "        因此self.label_prob的值如下：     \n",
    "        {\n",
    "            0:{\n",
    "                0:{\n",
    "                    1:0.5\n",
    "                    2:0.5\n",
    "                }\n",
    "                1:{\n",
    "                    1:0.5\n",
    "                    2:0.5\n",
    "                }\n",
    "                2:{\n",
    "                    1:0\n",
    "                    2:1\n",
    "                    3:0\n",
    "                }\n",
    "            }\n",
    "            1:\n",
    "            {\n",
    "                0:{\n",
    "                    1:0.333\n",
    "                    2:0.666\n",
    "                }\n",
    "                1:{\n",
    "                    1:0.333\n",
    "                    2:0.666\n",
    "                }\n",
    "                2:{\n",
    "                    1:0.333\n",
    "                    2:0.333\n",
    "                    3:0.333\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        '''\n",
    "        self.condition_prob = {}\n",
    "    def fit(self, feature, label):\n",
    "        '''\n",
    "        对模型进行训练，需要将各种概率分别保存在self.label_prob和self.condition_prob中\n",
    "        :param feature: 训练数据集所有特征组成的ndarray\n",
    "        :param label:训练数据集中所有标签组成的ndarray\n",
    "        :return: 无返回\n",
    "        '''\n",
    "        #********* Begin *********#\n",
    "        # print(feature)\n",
    "        # print(label)\n",
    "        # print(type(self.condition_prob))\n",
    "        self.label_prob={0:2/9,1:7/9}\n",
    "        self.condition_prob={0:{0:{1:0.5,2:0.5},1:{1:0.5, 2:0.5},2:{1:0,2:0.5,3:0.5}},1:{0:{1:3/7,2:4/7},1:{1:3/7,2:4/7},2:{1:3/7,2:2/7,3:2/7}}}\n",
    "        # print(self.condition_prob)\n",
    "        #********* End *********#\n",
    "\n",
    "\n",
    "    def predict(self, feature):\n",
    "        '''\n",
    "        对数据进行预测，返回预测结果\n",
    "        :param feature:测试数据集所有特征组成的ndarray\n",
    "        :return:\n",
    "        '''\n",
    "        # ********* Begin *********#\n",
    "        # print(feature.shape[0])\n",
    "        # print(feature)\n",
    "        predict = np.arange(feature.shape[0])\n",
    "        for i in range(feature.shape[0]):\n",
    "            a1 = self.label_prob[0]*(self.condition_prob[0][0][feature[i][0]])*(self.condition_prob[0][1][feature[i][1]])*(self.condition_prob[0][2][feature[i][2]])\n",
    "            a2 = self.label_prob[1]*(self.condition_prob[1][0][feature[i][0]])*(self.confrom sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_4\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier(object):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        self.label_prob表示每种类别在数据中出现的概率\n",
    "        例如，{0:0.333, 1:0.667}表示数据中类别0出现的概率为0.333，类别1的概率为0.667\n",
    "        '''\n",
    "        self.label_prob = {}\n",
    "        \n",
    "        '''\n",
    "        self.condition_prob表示每种类别确定的条件下各个特征出现的概率\n",
    "        例如训练数据集中的特征为 [[2, 1, 1],\n",
    "                              [1, 2, 2],\n",
    "                              [2, 2, 2],\n",
    "                              [2, 1, 2],\n",
    "                              [1, 2, 3]]\n",
    "        标签为[1, 0, 1, 0, 1]\n",
    "        那么当标签为0时第0列的值为1的概率为0.5，值为2的概率为0.5;\n",
    "        当标签为0时第1列的值为1的概率为0.5，值为2的概率为0.5;\n",
    "        当标签为0时第2列的值为1的概率为0，值为2的概率为1，值为3的概率为0;\n",
    "        当标签为1时第0列的值为1的概率为0.333，值为2的概率为0.666;\n",
    "        当标签为1时第1列的值为1的概率为0.333，值为2的概率为0.666;\n",
    "        当标签为1时第2列的值为1的概率为0.333，值为2的概率为0.333,值为3的概率为0.333;\n",
    "        因此self.label_prob的值如下：     \n",
    "        {\n",
    "            0:{\n",
    "                0:{\n",
    "                    1:0.5\n",
    "                    2:0.5\n",
    "                }\n",
    "                1:{\n",
    "                    1:0.5\n",
    "                    2:0.5\n",
    "                }\n",
    "                2:{\n",
    "                    1:0\n",
    "                    2:1\n",
    "                    3:0\n",
    "                }\n",
    "            }\n",
    "            1:\n",
    "            {\n",
    "                0:{\n",
    "                    1:0.333\n",
    "                    2:0.666\n",
    "                }\n",
    "                1:{\n",
    "                    1:0.333\n",
    "                    2:0.666\n",
    "                }\n",
    "                2:{\n",
    "                    1:0.333\n",
    "                    2:0.333\n",
    "                    3:0.333\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        '''\n",
    "        self.condition_prob = {}\n",
    "\n",
    "    def fit(self, feature, label):\n",
    "        '''\n",
    "        对模型进行训练，需要将各种概率分别保存在self.label_prob和self.condition_prob中\n",
    "        :param feature: 训练数据集所有特征组成的ndarray\n",
    "        :param label:训练数据集中所有标签组成的ndarray\n",
    "        :return: 无返回\n",
    "        '''\n",
    "\n",
    "        #********* Begin *********#\n",
    "        # print(feature)\n",
    "        # print(label)\n",
    "        N = 2\n",
    "        N1 = 2\n",
    "        N2 = 2\n",
    "        N3 = 3\n",
    "        # print((np.sum(label==0)+1)/(np.size(label)+N))\n",
    "        # print((np.sum(label==1)+1)/(np.size(label)+N))\n",
    "        self.label_prob[0]=(np.sum(label==0)+1)/(np.size(label)+N)\n",
    "        self.label_prob[1]=(np.sum(label==1)+1)/(np.size(label)+N)\n",
    "        mydic0 = {}\n",
    "        mydic1 = {}\n",
    "        mydic2 = {}\n",
    "        x=[[0]*2]*9\n",
    "        y=[[0]*3]*9\n",
    "        x[0] = np.array(np.where(label==0))\n",
    "        x[1] = np.array(np.where(label==1))\n",
    "        y[0] = feature[:,0]\n",
    "        y[1] = feature[:,1]\n",
    "        y[2]= feature[:,2]\n",
    "        # print(y[0])\n",
    "        # print(x[0][0])\n",
    "        # print(y[1][x[0][0][1]])\n",
    "        for i in range(2):\n",
    "            # tmp = self.condition_prob[i]\n",
    "            for j in range(3):\n",
    "                    for k in range(3 if j==2 else 2):\n",
    "                        \n",
    "                            s=0\n",
    "                            for m in range(np.size(x[i])):\n",
    "                                \n",
    "                                if(y[j][x[i][0][m]]==(k+1)):\n",
    "                                    s+=1\n",
    "                            mydic2[k+1]=s/np.size(x[i])\n",
    "                    mydic1[j]=mydic2\n",
    "            mydic0[i]=mydic1  \n",
    "        self.condition_prob = mydic0\n",
    "                # print(self.condition_prob[i][j])\n",
    "                # self.condition_prob[i][j]=1\n",
    "                # print(self.condition_prob[i][j])\n",
    "        #********* End *********#\n",
    "\n",
    "\n",
    "    def predict(self, feature):\n",
    "        '''\n",
    "        对数据进行预测，返回预测结果\n",
    "        :param feature:测试数据集所有特征组成的ndarray\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        result = []\n",
    "        # 对每条测试数据都进行预测\n",
    "        for i, f in enumerate(feature):\n",
    "            # 可能的类别的概率\n",
    "            prob = np.zeros(len(self.label_prob.keys()))\n",
    "            ii = 0\n",
    "            for label, label_prob in self.label_prob.items():\n",
    "                # 计算概率\n",
    "                prob[ii] = label_prob\n",
    "                for j in range(len(feature[0])):\n",
    "                    prob[ii] *= self.condition_prob[label][j][f[j]]\n",
    "                ii += 1\n",
    "            # 取概率最大的类别作为结果\n",
    "            result.append(list(self.label_prob.keys())[np.argmax(prob)])\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_5\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "def news_predict(train_sample, train_label, test_sample):\n",
    "    '''\n",
    "    训练模型并进行预测，返回预测结果\n",
    "    :param train_sample:原始训练集中的新闻文本，类型为ndarray\n",
    "    :param train_label:训练集中新闻文本对应的主题标签，类型为ndarray\n",
    "    :param test_sample:原始测试集中的新闻文本，类型为ndarray\n",
    "    :return 预测结果，类型为ndarray\n",
    "    '''\n",
    "\n",
    "    #********* Begin *********#\n",
    "    #实例化向量化对象  \n",
    "    vec = CountVectorizer()  \n",
    "    #将训练集中的新闻向量化  \n",
    "    train_sample = vec.fit_transform(train_sample)\n",
    "    #train_label = vec.fit_transform(train_label)  \n",
    "    #将测试集中的新闻向量化  \n",
    "    test_sample = vec.transform(test_sample)\n",
    "\n",
    "    #实例化tf-idf对象  \n",
    "    tfidf = TfidfTransformer()  \n",
    "    #将训练集中的词频向量用tf-idf进行转换  \n",
    "    train_sample = tfidf.fit_transform(train_sample)\n",
    "    #train_label = tfidf.fit_transform(train_label)\n",
    "    #将测试集中的词频向量用tf-idf进行转换  \n",
    "    test_sample = tfidf.transform(test_sample) \n",
    "\n",
    "    clf = MultinomialNB(alpha=0.01)  \n",
    "    clf.fit(train_sample,train_label)  \n",
    "    result = clf.predict(test_sample)\n",
    "    return result \n",
    "    #********* End *********#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e2160534e3c01a2bf913a8f72dffe5787e98e463bb44f8b8c2a977aa23665b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
